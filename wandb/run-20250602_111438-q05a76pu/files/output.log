WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
Using cache file path: cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather
Reading directly from cache cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather...
No se encontró la columna 'text'. Generando a partir de las demás columnas desde cache...
Tokenizando la columna 'text' para crear input_ids y attention_mask...
All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
[1mModel: "functional"[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m [0m[1mLayer (type)                 [0m[1m [0m┃[1m [0m[1mOutput Shape             [0m[1m [0m┃[1m [0m[1m        Param #[0m[1m [0m┃[1m [0m[1mConnected to              [0m[1m [0m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_ids ([38;5;33mInputLayer[0m)        │ ([38;5;45mNone[0m, [38;5;34m128[0m)               │               [38;5;34m0[0m │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ attention_mask ([38;5;33mInputLayer[0m)   │ ([38;5;45mNone[0m, [38;5;34m128[0m)               │               [38;5;34m0[0m │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ deberta ([38;5;33mLambda[0m)              │ ([38;5;45mNone[0m, [38;5;34m768[0m)               │               [38;5;34m0[0m │ input_ids[[38;5;34m0[0m][[38;5;34m0[0m],           │
│                               │                           │                 │ attention_mask[[38;5;34m0[0m][[38;5;34m0[0m]       │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ slice_last ([38;5;33mLambda[0m)           │ ([38;5;34m768[0m)                     │               [38;5;34m0[0m │ deberta[[38;5;34m0[0m][[38;5;34m0[0m]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ expand_dims ([38;5;33mLambda[0m)          │ ([38;5;34m1[0m, [38;5;34m768[0m)                  │               [38;5;34m0[0m │ slice_last[[38;5;34m0[0m][[38;5;34m0[0m]           │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ classification_mlp_0_128      │ ([38;5;34m1[0m, [38;5;34m128[0m)                  │          [38;5;34m98,432[0m │ expand_dims[[38;5;34m0[0m][[38;5;34m0[0m]          │
│ ([38;5;33mDense[0m)                       │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout ([38;5;33mDropout[0m)             │ ([38;5;34m1[0m, [38;5;34m128[0m)                  │               [38;5;34m0[0m │ classification_mlp_0_128[[38;5;34m…[0m │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ binary_classification_out     │ ([38;5;34m1[0m, [38;5;34m1[0m)                    │             [38;5;34m129[0m │ dropout[[38;5;34m0[0m][[38;5;34m0[0m]              │
│ ([38;5;33mDense[0m)                       │                           │                 │                            │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
[1m Total params: [0m[38;5;34m98,561[0m (385.00 KB)
[1m Trainable params: [0m[38;5;34m98,561[0m (385.00 KB)
[1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)
Building eval dataset...
Splitting dataset to featurewise...
Evaluation dataset is built!
Positive samples in eval set: 6
Negative samples in eval set: 20

Traceback (most recent call last):
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\main.py", line 88, in <module>
    (train_results, eval_results, final_epoch) = ft.evaluate(m, batch_size=128, epochs=5, steps_per_epoch=5, early_stopping_patience=5)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\framework\flow_transformer.py", line 646, in evaluate
    batch_results = m.train_on_batch(batch_X, batch_y)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 601, in train_on_batch
    logs = self.train_function(data())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 227, in function
    outputs = one_step_on_data(data)
              ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\util\traceback_utils.py", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 113, in one_step_on_data
    outputs = self.distribute_strategy.run(step_function, args=(data,))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 57, in train_step
    y_pred = self(x, training=True)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\traceback_utils.py", line 124, in error_handler
    del filtered_tb
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\implementations\input_encodings\huggingface_encoder.py", line 58, in hf_model_layer
    outputs = model(input_ids=ids, attention_mask=mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\utils\traceback_utils.py", line 72, in error_handler
    del filtered_tb
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_tf_utils.py", line 437, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 1262, in call
    outputs = self.deberta(
              ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_tf_utils.py", line 437, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 1105, in call
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 435, in call
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 327, in call
    attention_outputs = self.attention(
                        ^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 216, in call
    self_outputs = self.self(
                   ^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 731, in call
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 806, in disentangled_att_bias
    p2c_att = tf.matmul(key_layer, tf.transpose(pos_query_layer, [0, 1, 3, 2]))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'self' (type TFDebertaDisentangledSelfAttention).

{{function_node __wrapped____MklBatchMatMulV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[1024,12,128,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:BatchMatMulV2] name:

Call arguments received by layer 'self' (type TFDebertaDisentangledSelfAttention):
  • hidden_states=tf.Tensor(shape=(1024, 128, 768), dtype=float32)
  • attention_mask=tf.Tensor(shape=(1024, 1, 128, 128), dtype=uint8)
  • query_states=None
  • relative_pos=tf.Tensor(shape=(1, 128, 128), dtype=int64)
  • rel_embeddings=<tf.Variable 'tf_deberta_model_1/deberta/encoder/rel_embeddings.weight:0' shape=(1024, 768) dtype=float32, numpy=
array([[-2.5515855e-04,  4.5074557e-04, -1.8637361e-03, ...,
        -1.1294092e-03,  4.8300563e-03,  4.5189496e-05],
       [ 8.1727535e-02,  3.1886795e-03,  2.4229664e-02, ...,
        -3.2948174e-02,  6.5176077e-02, -3.2797638e-01],
       [-9.9139735e-02, -1.1789492e-02,  2.3481527e-02, ...,
         8.1517631e-03, -5.2460816e-02, -1.7639945e-01],
       ...,
       [ 2.5968602e-01,  9.8541724e-03, -1.2687260e-01, ...,
         2.6734494e-02,  3.4280643e-02,  1.4744590e-01],
       [ 2.7394652e-01, -2.1091852e-01,  8.9564249e-02, ...,
        -1.7702207e-02,  1.8076343e-02,  1.0582813e-01],
       [-3.4813622e-01,  1.7623873e-01, -1.2975909e-01, ...,
         9.2848845e-02,  1.9133857e-01,  5.2621339e-02]], dtype=float32)>
  • output_attentions=False
  • training=False
[0m
