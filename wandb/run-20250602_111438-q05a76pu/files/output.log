WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
Using cache file path: cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather
Reading directly from cache cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather...
No se encontrÃ³ la columna 'text'. Generando a partir de las demÃ¡s columnas desde cache...
Tokenizando la columna 'text' para crear input_ids y attention_mask...
All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
[1mModel: "functional"[0m
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m [0m[1mLayer (type)                 [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape             [0m[1m [0mâ”ƒ[1m [0m[1m        Param #[0m[1m [0mâ”ƒ[1m [0m[1mConnected to              [0m[1m [0mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ input_ids ([38;5;33mInputLayer[0m)        â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)               â”‚               [38;5;34m0[0m â”‚ -                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ attention_mask ([38;5;33mInputLayer[0m)   â”‚ ([38;5;45mNone[0m, [38;5;34m128[0m)               â”‚               [38;5;34m0[0m â”‚ -                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ deberta ([38;5;33mLambda[0m)              â”‚ ([38;5;45mNone[0m, [38;5;34m768[0m)               â”‚               [38;5;34m0[0m â”‚ input_ids[[38;5;34m0[0m][[38;5;34m0[0m],           â”‚
â”‚                               â”‚                           â”‚                 â”‚ attention_mask[[38;5;34m0[0m][[38;5;34m0[0m]       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ slice_last ([38;5;33mLambda[0m)           â”‚ ([38;5;34m768[0m)                     â”‚               [38;5;34m0[0m â”‚ deberta[[38;5;34m0[0m][[38;5;34m0[0m]              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ expand_dims ([38;5;33mLambda[0m)          â”‚ ([38;5;34m1[0m, [38;5;34m768[0m)                  â”‚               [38;5;34m0[0m â”‚ slice_last[[38;5;34m0[0m][[38;5;34m0[0m]           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ classification_mlp_0_128      â”‚ ([38;5;34m1[0m, [38;5;34m128[0m)                  â”‚          [38;5;34m98,432[0m â”‚ expand_dims[[38;5;34m0[0m][[38;5;34m0[0m]          â”‚
â”‚ ([38;5;33mDense[0m)                       â”‚                           â”‚                 â”‚                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout ([38;5;33mDropout[0m)             â”‚ ([38;5;34m1[0m, [38;5;34m128[0m)                  â”‚               [38;5;34m0[0m â”‚ classification_mlp_0_128[[38;5;34mâ€¦[0m â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ binary_classification_out     â”‚ ([38;5;34m1[0m, [38;5;34m1[0m)                    â”‚             [38;5;34m129[0m â”‚ dropout[[38;5;34m0[0m][[38;5;34m0[0m]              â”‚
â”‚ ([38;5;33mDense[0m)                       â”‚                           â”‚                 â”‚                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1m Total params: [0m[38;5;34m98,561[0m (385.00 KB)
[1m Trainable params: [0m[38;5;34m98,561[0m (385.00 KB)
[1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)
Building eval dataset...
Splitting dataset to featurewise...
Evaluation dataset is built!
Positive samples in eval set: 6
Negative samples in eval set: 20

Traceback (most recent call last):
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\main.py", line 88, in <module>
    (train_results, eval_results, final_epoch) = ft.evaluate(m, batch_size=128, epochs=5, steps_per_epoch=5, early_stopping_patience=5)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\framework\flow_transformer.py", line 646, in evaluate
    batch_results = m.train_on_batch(batch_X, batch_y)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 601, in train_on_batch
    logs = self.train_function(data())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 227, in function
    outputs = one_step_on_data(data)
              ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\util\traceback_utils.py", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 113, in one_step_on_data
    outputs = self.distribute_strategy.run(step_function, args=(data,))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 57, in train_step
    y_pred = self(x, training=True)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\traceback_utils.py", line 124, in error_handler
    del filtered_tb
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\implementations\input_encodings\huggingface_encoder.py", line 58, in hf_model_layer
    outputs = model(input_ids=ids, attention_mask=mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\utils\traceback_utils.py", line 72, in error_handler
    del filtered_tb
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_tf_utils.py", line 437, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 1262, in call
    outputs = self.deberta(
              ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_tf_utils.py", line 437, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 1105, in call
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 435, in call
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 327, in call
    attention_outputs = self.attention(
                        ^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 216, in call
    self_outputs = self.self(
                   ^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 731, in call
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 806, in disentangled_att_bias
    p2c_att = tf.matmul(key_layer, tf.transpose(pos_query_layer, [0, 1, 3, 2]))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.ResourceExhaustedError: Exception encountered when calling layer 'self' (type TFDebertaDisentangledSelfAttention).

{{function_node __wrapped____MklBatchMatMulV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[1024,12,128,256] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:BatchMatMulV2] name:

Call arguments received by layer 'self' (type TFDebertaDisentangledSelfAttention):
  â€¢ hidden_states=tf.Tensor(shape=(1024, 128, 768), dtype=float32)
  â€¢ attention_mask=tf.Tensor(shape=(1024, 1, 128, 128), dtype=uint8)
  â€¢ query_states=None
  â€¢ relative_pos=tf.Tensor(shape=(1, 128, 128), dtype=int64)
  â€¢ rel_embeddings=<tf.Variable 'tf_deberta_model_1/deberta/encoder/rel_embeddings.weight:0' shape=(1024, 768) dtype=float32, numpy=
array([[-2.5515855e-04,  4.5074557e-04, -1.8637361e-03, ...,
        -1.1294092e-03,  4.8300563e-03,  4.5189496e-05],
       [ 8.1727535e-02,  3.1886795e-03,  2.4229664e-02, ...,
        -3.2948174e-02,  6.5176077e-02, -3.2797638e-01],
       [-9.9139735e-02, -1.1789492e-02,  2.3481527e-02, ...,
         8.1517631e-03, -5.2460816e-02, -1.7639945e-01],
       ...,
       [ 2.5968602e-01,  9.8541724e-03, -1.2687260e-01, ...,
         2.6734494e-02,  3.4280643e-02,  1.4744590e-01],
       [ 2.7394652e-01, -2.1091852e-01,  8.9564249e-02, ...,
        -1.7702207e-02,  1.8076343e-02,  1.0582813e-01],
       [-3.4813622e-01,  1.7623873e-01, -1.2975909e-01, ...,
         9.2848845e-02,  1.9133857e-01,  5.2621339e-02]], dtype=float32)>
  â€¢ output_attentions=False
  â€¢ training=False
[0m
