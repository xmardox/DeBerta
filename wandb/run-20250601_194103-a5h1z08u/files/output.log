WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
Using cache file path: cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather
Reading directly from cache cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather...
All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
Traceback (most recent call last):
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\main.py", line 79, in <module>
    m = ft.build_model()
        ^^^^^^^^^^^^^^^^
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\framework\flow_transformer.py", line 104, in build_model
    m_x = self.sequential_model.apply(m_x, prefix)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\implementations\transformers\named_transformers.py", line 118, in apply
    raise ValueError("El input X debe contener input_ids y attention_mask")
ValueError: El input X debe contener input_ids y attention_mask
[0m
