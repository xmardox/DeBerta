WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
Using cache file path: cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather
Reading directly from cache cache\CSE_CIC_IDS_1300_QdLmZHuh8yOmlGcKBEkf7hepImY0_9Xii0x0FeiUOoDMwEgiWksBPgKc0.feather...
No se encontró la columna 'text'. Generando a partir de las demás columnas desde cache...
Tokenizando la columna 'text' para crear input_ids y attention_mask...
All model checkpoint layers were used when initializing TFDebertaModel.

All the layers of TFDebertaModel were initialized from the model checkpoint at microsoft/deberta-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDebertaModel for predictions without further training.
WARNING:tensorflow:From C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
[1mModel: "functional"[0m
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m [0m[1mLayer (type)                 [0m[1m [0m┃[1m [0m[1mOutput Shape             [0m[1m [0m┃[1m [0m[1m        Param #[0m[1m [0m┃[1m [0m[1mConnected to              [0m[1m [0m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ input_ids ([38;5;33mInputLayer[0m)        │ ([38;5;45mNone[0m, [38;5;34m128[0m)               │               [38;5;34m0[0m │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ attention_mask ([38;5;33mInputLayer[0m)   │ ([38;5;45mNone[0m, [38;5;34m128[0m)               │               [38;5;34m0[0m │ -                          │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ deberta ([38;5;33mLambda[0m)              │ ([38;5;45mNone[0m, [38;5;34m768[0m)               │               [38;5;34m0[0m │ input_ids[[38;5;34m0[0m][[38;5;34m0[0m],           │
│                               │                           │                 │ attention_mask[[38;5;34m0[0m][[38;5;34m0[0m]       │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ slice_last ([38;5;33mLambda[0m)           │ ([38;5;34m768[0m)                     │               [38;5;34m0[0m │ deberta[[38;5;34m0[0m][[38;5;34m0[0m]              │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ expand_dims ([38;5;33mLambda[0m)          │ ([38;5;34m1[0m, [38;5;34m768[0m)                  │               [38;5;34m0[0m │ slice_last[[38;5;34m0[0m][[38;5;34m0[0m]           │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ classification_mlp_0_128      │ ([38;5;34m1[0m, [38;5;34m128[0m)                  │          [38;5;34m98,432[0m │ expand_dims[[38;5;34m0[0m][[38;5;34m0[0m]          │
│ ([38;5;33mDense[0m)                       │                           │                 │                            │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ dropout ([38;5;33mDropout[0m)             │ ([38;5;34m1[0m, [38;5;34m128[0m)                  │               [38;5;34m0[0m │ classification_mlp_0_128[[38;5;34m…[0m │
├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤
│ binary_classification_out     │ ([38;5;34m1[0m, [38;5;34m1[0m)                    │             [38;5;34m129[0m │ dropout[[38;5;34m0[0m][[38;5;34m0[0m]              │
│ ([38;5;33mDense[0m)                       │                           │                 │                            │
└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘
[1m Total params: [0m[38;5;34m98,561[0m (385.00 KB)
[1m Trainable params: [0m[38;5;34m98,561[0m (385.00 KB)
[1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)
Building eval dataset...
Splitting dataset to featurewise...
Evaluation dataset is built!
Positive samples in eval set: 6
Negative samples in eval set: 20

Traceback (most recent call last):
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\main.py", line 88, in <module>
    (train_results, eval_results, final_epoch) = ft.evaluate(m, batch_size=128, epochs=5, steps_per_epoch=5, early_stopping_patience=5)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\framework\flow_transformer.py", line 646, in evaluate
    batch_results = m.train_on_batch(batch_X, batch_y)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 601, in train_on_batch
    logs = self.train_function(data())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 227, in function
    outputs = one_step_on_data(data)
              ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tensorflow\python\util\traceback_utils.py", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 113, in one_step_on_data
    outputs = self.distribute_strategy.run(step_function, args=(data,))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py", line 57, in train_step
    y_pred = self(x, training=True)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\traceback_utils.py", line 124, in error_handler
    del filtered_tb
  File "c:\Users\lapla\Downloads\FlowTransformer-master\FlowTransformer-master\implementations\input_encodings\huggingface_encoder.py", line 55, in hf_model_layer
    return model(input_ids=ids, attention_mask=mask).last_hidden_state[:, 0, :]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\tf_keras\src\utils\traceback_utils.py", line 72, in error_handler
    del filtered_tb
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_tf_utils.py", line 437, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 1262, in call
    outputs = self.deberta(
              ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\modeling_tf_utils.py", line 437, in run_call_with_unpacked_inputs
    return func(self, **unpacked_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 1105, in call
    encoder_outputs = self.encoder(
                      ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 435, in call
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 327, in call
    attention_outputs = self.attention(
                        ^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 216, in call
    self_outputs = self.self(
                   ^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 731, in call
    rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 791, in disentangled_att_bias
    c2p_att = torch_gather(c2p_att, c2p_dynamic_expand(c2p_pos, query_layer, relative_pos), -1)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lapla\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\transformers\models\deberta\modeling_tf_deberta.py", line 521, in torch_gather
    if gather_axis != tf.rank(x) - 1:
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: Exception encountered when calling layer 'self' (type TFDebertaDisentangledSelfAttention).

Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.

Call arguments received by layer 'self' (type TFDebertaDisentangledSelfAttention):
  • hidden_states=tf.Tensor(shape=(1024, 128, 768), dtype=float32)
  • attention_mask=tf.Tensor(shape=(1024, 1, 128, 128), dtype=uint8)
  • query_states=None
  • relative_pos=tf.Tensor(shape=(1, 128, 128), dtype=int64)
  • rel_embeddings=<tf.Variable 'tf_deberta_model_1/deberta/encoder/rel_embeddings.weight:0' shape=(1024, 768) dtype=float32>
  • output_attentions=False
  • training=False
[0m
